Scenario



source_rdd = spark.sparkContext.parallelize([
    (1, "A"),
    (2, "B"),
    (3, "C"),
    (4, "D")
],1)

target_rdd = spark.sparkContext.parallelize([
    (1, "A"),
    (2, "B"),
    (4, "X"),
    (5, "F")
],2)

# Convert RDDs to DataFrames using toDF()
df1 = source_rdd.toDF(["id", "name"])
df2 = target_rdd.toDF(["id", "name1"])

# Show the DataFrames
df1.show()
df2.show()



print()
print("====FULL JOIN====")
print()

fulljoin = df1.join(df2, ["id"], "full")
fulljoin.show()


print()
print("===Condition====")
print()

from pyspark.sql.functions import *

condition = fulljoin.withColumn("comment",expr("case when name=name1 then 'match' else 'mismatch' end"))

condition.show()



print()
print("===Filter Condition====")
print()


filter = condition.filter("comment !='match'")
filter.show()

print()
print("===null Condition====")
print()

nullcondition = filter.withColumn("comment",expr("""
                                                case when name1 is null then 'New in Source'
                                                when name is null then 'new in Target'
                                                else
                                                comment
                                                end


                                                    """))

nullcondition.show()


print()
print("===Final df====")
print()


finaldf = nullcondition.drop("name","name1")
finaldf.show()