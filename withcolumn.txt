Full withColumn Code



data = [
    ("00000000", "06-26-2011", 200, "Exercise", "GymnasticsPro", "cash"),
    ("00000001", "05-26-2011", 300, "Exercise", "Weightlifting", "credit"),
    ("00000002", "06-01-2011", 100, "Exercise", "GymnasticsPro", "cash"),
    ("00000003", "06-05-2011", 100, "Gymnastics", "Rings", "credit"),
    ("00000004", "12-17-2011", 300, "Team Sports", "Field", "paytm"),
    ("00000005", "02-14-2011", 200, "Gymnastics", None, "cash")
]

rdd = spark.sparkContext.parallelize(data)
columns = ["txnno", "txndate", "amount", "category", "product", "spendby"]
csvdf = rdd.toDF(columns)
print()
print("===== raw dataframe ===")
print()

csvdf.show()


print()
print("=====EXPRESSIONS=====")
print()


from pyspark.sql.functions import *


#.SelectExpr("case when spendby ='cash' then 0 else 1 end as status")



print()
print("=====EXPRESSIONS=====")
print()


from pyspark.sql.functions import *


#.SelectExpr("case when spendby ='cash' then 0 else 1 end as status")

procdf = (

    csvdf.withColumn( "category" , expr("upper(category)"))
    .withColumn("amount", expr("amount+100"))
    .withColumn("txnno",expr("cast(txnno as int)"))
    .withColumn("txndate",expr("split(txndate,'-')[2]"))
    .withColumn("product",expr("lower(product)"))
    .withColumn("spendby",expr("spendby"))
    .withColumn("status",expr("case when spendby ='cash' then 0 else 1 end"))
    .withColumn("con",expr("concat(category,'~zeyo')"))

)

procdf.show()



******* withcoumnReanamed



from pyspark.sql.functions import *


#.SelectExpr("case when spendby ='cash' then 0 else 1 end as status")

procdf = (

    csvdf.withColumn("txndate",expr("split(txndate,'-')[2]"))
         .withColumnRenamed("txndate","year")


)

procdf.show()